---
title: "build_restoration_and_controls"
author: "Lucy Andrews"
date: "11/29/2021"
output: html_document
---

# Set up

## Load packages

```{r}
# load packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(janitor))
suppressPackageStartupMessages(library(foreach))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(googledrive))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(spdep))
suppressPackageStartupMessages(library(raster))
suppressPackageStartupMessages(library(elevatr))
suppressPackageStartupMessages(library(esri2sf))
suppressPackageStartupMessages(library(exactextractr))
suppressPackageStartupMessages(library(fst))
suppressPackageStartupMessages(library(rgdal))
suppressPackageStartupMessages(library(units))
suppressPackageStartupMessages(library(tigris))
suppressPackageStartupMessages(library(tidycensus))
suppressPackageStartupMessages(library(rmapshaper))
suppressPackageStartupMessages(library(ggnewscale))
suppressPackageStartupMessages(library(caladaptr))
suppressPackageStartupMessages(library(nhdR))
suppressPackageStartupMessages(library(nhdplusTools))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(MatchIt))
suppressPackageStartupMessages(library(survival))
```

## Set global options

```{r}
# set global options
options(stringsAsFactors = FALSE)
options(tigris_use_cache = TRUE)
global_crs <- st_crs(4269)
options(scipen = 999)
options(timeout = 30000)
drop_ftypes <- c("Coastline", "Pipeline", "Connector")
```

## Build custom functions

```{r}
# build custom functions and operators
# %notin%: operator that returns the negation of %in%
`%notin%` <- Negate(`%in%`)

# build function that pulls CA boundary into global environment as sf object
get_ca_boundary <- function(crs) {
    states() %>%
      filter(NAME == "California") %>%
      rename(name = NAME) %>%
      dplyr::select(name) %>%
      st_transform(crs = crs)
}

# build function that quickly pulls up sf object attribute tables
view_flat <- function(sf_obj) {
  st_drop_geometry(sf_obj) %>%
    View(.)
}

# build function that returns the midpoint of a given sf line object
st_line_midpoints <- function(sf_lines = NULL) {
  
  g <- st_geometry(sf_lines)
  
  g_mids <- lapply(g, function(x) {
    
    coords <- as.matrix(x)
    
    get_mids <- function (coords) {
      dist <- sqrt((diff(coords[, 1])^2 + (diff(coords[, 2]))^2))
      dist_mid <- sum(dist)/2
      dist_cum <- c(0, cumsum(dist))
      end_index <- which(dist_cum > dist_mid)[1]
      start_index <- end_index - 1
      start <- coords[start_index, ]
      end <- coords[end_index, ]
      dist_remaining <- dist_mid - dist_cum[start_index]
      mid <- start + (end - start) * (dist_remaining/dist[start_index])
      return(mid)
    
    }
    
    mids <- st_point(get_mids(coords))
    
    })
  
  geometry <- st_sfc(g_mids, crs = st_crs(sf_lines))
  
  geometry <- st_sf(geometry)
  
}

# build a function that computes total stream length upstream of a segment
get_upstream_length <- function(comid, ut_network_df) {
  
  try(upstream_comids <- get_UT(ut_network_df, comid, distance = NULL))
  
  upstream_length_km <- ut_network_df %>%
    filter(COMID %in% upstream_comids) %>%
    summarize(total_length_km = sum(LENGTHKM)) %>%
    pull(total_length_km)
  
  return(upstream_length_km)
  
}
```

## Load California state boundary

```{r}
# load California state boundary
ca_boundary <- get_ca_boundary(crs = global_crs)
```



# Load restoration data

## Create directories

```{r}
# create directories
if(!dir.exists(here("raw_data", "restoration_areas"))) {
  dir.create(here("raw_data", "restoration_areas"))
}

if(!dir.exists(here("raw_data", "restoration_projects"))) {
  dir.create(here("raw_data", "restoration_projects"))
}
```

## Download restoration files

```{r}
# specify urls
restoration_areas_url <- "https://filelib.wildlife.ca.gov/Public/BDB/GIS/BIOS/Public_Datasets/700_799/ds734.zip"

restoration_projects_url <- "https://filelib.wildlife.ca.gov/Public/BDB/GIS/BIOS/Public_Datasets/100_199/ds168.zip"

# download files
if(!file.exists(here("raw_data", "restoration_areas", "cdfw_restoration_areas.zip"))) {
  download.file(url = restoration_areas_url,
                destfile = here("raw_data", "restoration_areas", "cdfw_restoration_areas.zip"))
}

if(!file.exists(here("raw_data", "restoration_projects", "cdfw_restoration_projects.zip"))) {
  download.file(url = restoration_projects_url,
                destfile = here("raw_data", "restoration_projects", "cdfw_restoration_projects.zip"))
}

# unzip files
unzip(zipfile = here("raw_data", "restoration_areas", "cdfw_restoration_areas.zip"),
      overwrite = TRUE,
      exdir = here("raw_data", "restoration_areas", "cdfw_restoration_areas"))

unzip(zipfile = here("raw_data", "restoration_projects", "cdfw_restoration_projects.zip"),
      overwrite = TRUE,
      exdir = here("raw_data", "restoration_projects", "cdfw_restoration_projects"))

# clean up
rm(restoration_areas_url, restoration_projects_url)
```

## Read in restoration data

### Read in restoration areas

```{r}
# read in and clean restoration areas sf object - multipolygon geometry
restoration_areas <- st_read(dsn = here("raw_data", "restoration_areas", "cdfw_restoration_areas")) %>%
  st_transform(crs = global_crs) %>%
  rename(huc2 = HUC_2,
         huc4 = HUC_4,
         huc6 = HUC_6,
         huc8 = HUC_8,
         huc8_state = HU_8_STATE,
         fips_code = FIPS_C,
         huc10_name = HU_10_NAME) %>%
  rename_with(tolower) %>%
  ms_simplify(keep = 0.05) %>%
  mutate(is_valid = st_is_valid(geometry)) %>%
  filter(is_valid) %>%
  dplyr::select(-is_valid)

restoration_areas_diss <- st_union(x = restoration_areas) %>%
  st_as_sf()

# check out map of project areas
ggplot() +
  geom_sf(data = ca_boundary) +
  geom_sf(data = restoration_areas_diss, fill = "yellowgreen") +
  theme_minimal()
```

### Read in restoration projects

```{r}
# read in and clean restoration projects sf object - point geometry
### ---> NOTE: Lucy chose not to use this dataset, as it's one-to-many with
###            the restoration_project_ex dataset below, which is more useful
###            for our purposes
restoration_worksites <- st_read(dsn = here("raw_data", "restoration_projects",
                                            "cdfw_restoration_projects", "ds168.gdb"),
                                layer = "ds168") %>%
  st_as_sf() %>%
  st_transform(crs = global_crs) %>%
  rename(project_id = ProjectID,
         worksite_id = WorksiteID,
         worksite_name = WorksiteNa,
         award_year = Award_Year,
         worksite_latitude = WorksiteLa,
         worksite_longitude = WorksiteLo,
         geometry = Shape)

# read in and clean restoration projects additional fields - dataframe
restoration_projects <- st_read(dsn = here("raw_data", "restoration_projects",
                                           "cdfw_restoration_projects", "ds168.gdb"),
                                   layer = "ds168_ex") %>%
  rename(project_id = ProjectID) %>%
  rename_with(tolower) %>%
  st_as_sf(coords = c("center_longitude", "center_latitude"),
           crs = global_crs)

# identify project characteristics to drop
drop_project_type <- c("Water Measuring Devices",
                       "Monitoring Watershed Restoration",
                       "AmericCorps",
                       "Project Design",
                       "Public Involvement and Capacity Building",
                       "Private Sector Technical Training and Education",
                       "Public School Watershed and Fishery Conservation Education Project",
                       "Cooperative Rearing",
                       "Watershed and Regional Organization",
                       "Watershed Evaluation, Assessment, Planning",
                       "Monitoring Status and Trends")

drop_cal_work_status <- c("Terminated/Cancelled",
                          "TerminateCancel")

drop_grant_status <- c("Cancelnofundsspent")

# join additional fields to restoration projects sf object
restoration_projects <- restoration_projects %>%
  filter(project_type %notin% drop_project_type,
         cal_work_status %notin% drop_cal_work_status,
         grant_status %notin% drop_grant_status)

# clean up
rm(restoration_worksites, drop_project_type, drop_cal_work_status, drop_grant_status)
```



# Load NHD data

## Download and clean flowlines

```{r}
# create directory
if(!dir.exists(here("raw_data", "nhd_flowlines"))) {
  dir.create(here("raw_data", "nhd_flowlines"))
}

# set nhdR download and directory paths
Sys.setenv(nhdR_path = here("raw_data", "nhd_flowlines"))

# download flowlines and attribute data for VPUs 17 (PNW) and 18 (CA)
suppressMessages(nhd_plus_get(vpu = 17, component = "NHDSnapshot"))
suppressMessages(nhd_plus_get(vpu = 18, component = "NHDSnapshot"))
suppressMessages(nhd_plus_get(vpu = 17, "NHDPlusAttributes"))
suppressMessages(nhd_plus_get(vpu = 18, "NHDPlusAttributes"))

# read in flowlines
flowlines_17 <- nhd_plus_load(vpu = 17,
                              component = "NHDSnapshot",
                              dsn = "NHDFlowline") %>%
  st_transform(crs = global_crs)

flowlines_18 <- nhd_plus_load(vpu = 18,
                              component = "NHDSnapshot",
                              dsn = "NHDFlowline") %>%
  st_transform(crs = global_crs)

# create a single flowlines object, clean fields names,
# and filter for intersection with CDFW project areas
flowlines_all <- rbind(flowlines_17, flowlines_18) %>%
  rename(length_km = LENGTHKM) %>%
  rename_with(.fn = tolower) %>%
  dplyr::select(comid, gnis_id, gnis_name, length_km, ftype) %>%
  filter(ftype %notin% drop_ftypes)

flowlines_study <- flowlines_all %>%
  st_join(y = restoration_areas_diss, join = st_intersects, left = FALSE)

# clean up
rm(flowlines_17, flowlines_18)
```



# Sample control points 

```{r}
# sample midpoints with comid as the unique identifier
# and add a field identifying these sf objects as control points
control_points <- flowlines_study %>%
  dplyr::select(comid) %>%
  st_line_midpoints() %>%
  cbind(flowlines_study$comid) %>%
  rename(obs_id = flowlines_study.comid) %>%
  mutate(obs_type = "control")

# drop control points co-located with restoration sites
# HERE!
```



# Create a single dataset of points of interest (control and restored)

## Combine restoration sites and control sites to create a single dataframe of study points

```{r}
study_points <- restoration_projects %>%
  dplyr::select(project_id, geometry) %>%
  rename(obs_id = project_id) %>%
  mutate(obs_type = "restoration_site") %>%
  rbind(control_points)
```

## Filter study points for intersection with incorporated and census-designated places

```{r}
# load census bureau "places" sf polygon objects
incorp_cdp <- places(state = "CA", year = 2020) %>%
  rename(place_fip = PLACEFP,
         geo_id = GEOID,
         name = NAME,
         name_w_type = NAMELSAD,
         type_code = LSAD,
         fips_code = CLASSFP,
         area_land = ALAND,
         area_water = AWATER) %>%
  dplyr::select(place_fip, geo_id, name, name_w_type, type_code,
                fips_code, area_land, area_water)

# filter points of interest for spatial intersection with incorporated and
# census-designated places
study_points <- st_join(x = study_points,
                        y = dplyr::select(incorp_cdp, geometry),
                        join = st_intersects,
                        left = FALSE)

# clean up
rm(control_points, incorp_cdp)
```

## Visualize control points and restoration points

```{r}
# generate a map of control points and restoration points
ggplot() +
  geom_sf(data = ca_boundary) +
  geom_sf(data = study_points, aes(color = obs_type), size = 0.5) +
  theme_minimal()
```



# Enrich study points with project and hydroecological data

## Associate study points with project attributes

```{r}
# drop duplicate study points
study_points <- study_points %>%
  distinct(geometry, .keep_all = TRUE) %>%
  left_join(y = st_drop_geometry(dplyr::select(restoration_projects, project_id,
                                               award_year, project_type, agency,
                                               amount_approved)),
            by = c("obs_id" = "project_id"))
```

## Associate study points with flowlines and flowline attributes

### Import flowlines value-added attributes (VAAs)

```{r}
# download value-added attributes (original source: Hydroshare)
drive_download(file = as_id("1tpMnQDXD50jYQ5EtZJthOAKPc5squEn0"),
               path = here("raw_data", "nhd_flowlines", "vaa.zip"),
               overwrite = TRUE)

# unzip value-added attributes
unzip(zipfile = here("raw_data", "nhd_flowlines", "vaa.zip"),
      exdir = here("raw_data", "nhd_flowlines"))

# read in value-added attributes and filter for intersection with California
# this is needed to calculate upstream attributes that may extend beyond CDFW territory
vaa <- read_fst(path = here("raw_data", "nhd_flowlines", "nhdplusVAA.fst", "nhdplusVAA.fst")) %>%
  filter(comid %in% flowlines_all$comid,
         ftype %notin% drop_ftypes) %>%
  rename(stream_order = streamorde,
         length_km = lengthkm,
         from_node = fromnode,
         to_node = tonode,
         level_path = levelpathi,
         path_length = pathlength,
         terminal_path = terminalpa,
         up_level_path = uplevelpat,
         up_hydroseq = uphydroseq,
         down_level_path = dnlevelpat,
         down_hydroseq = dnhydroseq,
         area_sqkm = areasqkm,
         tot_da_sqkm = totdasqkm) %>%
  dplyr::select(comid, length_km, stream_order, from_node, to_node, hydroseq,
                level_path, path_length, terminal_path, up_level_path, up_hydroseq,
                down_level_path, down_hydroseq, slope, tot_da_sqkm)

# add drainage area from value-added attributes to flowlines
flowlines_all <- left_join(x = flowlines_all, y = vaa, by = "comid")
```

### Add flowline and flowline value-added attributes to study points

```{r}
# join study points to flowlines based on nearest feature
study_points <- study_points %>%
  st_join(y = dplyr::select(flowlines_all, comid, ftype, stream_order, slope, tot_da_sqkm),
          join = st_nearest_feature)
```

## Add upstream watershed characteristics

```{r}
# create an upstream tributary network (edge list with specified column names)
ut_network <- vaa %>%
  rename(COMID = comid,
         Pathlength = path_length,
         LENGTHKM = length_km,
         Hydroseq = hydroseq,
         LevelPathI = level_path,
         DnHydroseq = down_hydroseq) %>%
  dplyr::select(COMID, Pathlength, LENGTHKM, Hydroseq, LevelPathI, DnHydroseq)

# compute upstream stream length (including tributaries) for each study point
study_points <- study_points %>%
  rowwise() %>%
  mutate(upstream_length_km = get_upstream_length(comid = comid,
                                                  ut_network_df = ut_network),
         drainage_density = upstream_length_km / tot_da_sqkm) %>%
  ungroup()

# clean up missing data
study_points <- study_points %>%
  mutate(slope = case_when(slope == -9998 ~ NA_real_,
                            TRUE ~ slope),
         drainage_density = case_when(is.infinite(drainage_density) ~ NA_real_,
                                       TRUE ~ drainage_density))
```

## Add HUC12 and CDFW Areas of Conservation Emphasis attributes

```{r}
# create directory
if(!dir.exists(here("raw_data", "ace"))) {
  dir.create(here("raw_data", "ace"))
}

# specify url
ace_url <- "https://filelib.wildlife.ca.gov/Public/BDB/ACE/ACE_Summary_Datasets.zip"

# download file
if(!file.exists(here("raw_data", "ace", "ace_summary_datasets.zip"))) {
  download.file(url = ace_url,
                destfile = here("raw_data", "ace", "ACE_Summary_Datasets.zip"))
}

# unzip file
unzip(here("raw_data", "ace", "ACE_Summary_Datasets.zip"),
      exdir = here("raw_data", "ace"))

# read in and clean ACE dataset
huc12 <- readOGR(dsn = here("raw_data", "ace", "ACE_Summary_Datasets", "ds2743.gdb")) %>%
  st_as_sf() %>%
  st_transform(crs = global_crs) %>%
  ms_simplify(keep = 0.05) %>%
  st_intersection(y = ca_boundary) %>%
  rename(huc12 = HUC12,
         huc12_name = Name,
         native_aquatic_rank = NtvAqRankSW,
         native_aquatic_index = NtvAqSumSW,
         native_fish_count = NtvFish,
         native_aquatic_invert_count = NtvAqInvt,
         native_aquatic_amphib_count = NtvAqAmph,
         native_aquatic_reptile_count = NtvAqRept) %>%
  mutate(huc12_area_sqkm = drop_units(set_units(st_area(geometry), "km^2"))) %>%
  dplyr::select(huc12, huc12_name, huc12_area_sqkm,
                native_aquatic_rank, native_aquatic_index, native_fish_count,
                native_aquatic_invert_count, native_aquatic_amphib_count,
                native_aquatic_reptile_count)

# enrich restoration projects with HUC12 and ACE
study_points <- st_join(study_points, huc12, join = st_intersects) 

# clean up
rm(ace_url)
```

## Add environmental flows

```{r}
# create directory
if(!dir.exists(here("raw_data", "eflows"))) {
  dir.create(here("raw_data", "eflows"))
}

# specify eflows url
eflows_url <- "https://s3-us-west-1.amazonaws.com/funcflow/resources/eflow_geodatabase.zip"

if(!file.exists(here("raw_data", "eflows", "eflows.zip"))) {
  download.file(url = eflows_url,
                destfile = here("raw_data", "eflows", "eflows.zip"))
}

# unzip eflows data
unzip(zipfile = here("raw_data", "eflows", "eflows.zip"),
      exdir = here("raw_data", "eflows"))

# read in eflows data
eflows <- st_read(here("raw_data", "eflows", "Final_Classification_9CLASS",
                       "Final_Classification_9CLASS.shp")) %>%
  st_transform(crs = global_crs) %>%
  rename(eflows_class = CLASS,
         comid = COMID) %>%
  mutate(eflows_class = as.character(eflows_class)) %>%
  dplyr::select(eflows_class, comid) %>%
  distinct(comid, .keep_all = TRUE)

# add eflows to restoration projects
study_points <- left_join(study_points, st_drop_geometry(eflows),
                          by = "comid")

# clean up
rm(eflows, eflows_url)
```

## Add precipitation

```{r}
if(FALSE) {
  # get grid of precip data coverage
  loca <- ca_locagrid_geom() %>%
    st_transform(crs = global_crs) %>%
    rename(loca_id = id)
  
  # trim restoration projects dataset and
  # specify coordinates of restoration projects for API call
  study_points_coords <- st_join(study_points, loca, left = FALSE) %>%
    st_coordinates() %>%
    as_tibble() %>%
    cbind(study_points$obs_id) %>%
    rename(obs_id = "study_points$obs_id",
           x = X,
           y = Y) %>%
    rowid_to_column(var = "index")
    
  # generate a precip API call
  precip_api_call <- ca_loc_pt(x = ca_apireq(),
                               dplyr::select(study_points_coords, x, y),
                               id = study_points_coords$obs_id) %>%
    ca_gcm("ens32avg") %>%
    ca_scenario("historical") %>%
    ca_cvar("pr") %>%
    ca_period("year") %>%
    ca_years(start = 1980, end = 2005)
  
  # make API call for precip data
  precip <- precip_api_call %>%
    ca_getvals_tbl(quiet = TRUE)
  
  rm(loca, precip_api_call, study_points_coords)
}
```

Easier precip data source: UC Berkeley geolibrary
https://geodata.lib.berkeley.edu/catalog/stanford-fs379qr8881 
Original source: PRISM
Annual normal, 1971-2000

```{r}
# create directory
if(!dir.exists(here("raw_data", "precip"))) {
  dir.create(here("raw_data", "precip"))
}

# specify url
precip_url <- "https://stacks.stanford.edu/file/druid:fs379qr8881/data.zip"

# download zipped file of precip data
if(!file.exists(here("raw_data", "precip", "precip.zip"))) {
  download.file(url = precip_url,
                destfile = here("raw_data", "precip", "precip.zip"))
}

# unzip precip data
unzip(zipfile = here("raw_data", "precip", "precip.zip"),
      exdir = here("raw_data", "precip"))

# read in precip raster
precip <- raster(here("raw_data", "precip", "cappt7100_14", "hdr.adf"))

# grab precip value for each restoration site
precip_extract <- extract(precip, study_points) %>%
  as_tibble() %>%
  rename(precip = value)

# bind to study points
study_points <- cbind(study_points, precip_extract)

# clean up
rm(precip_url, precip, precip_extract)
```

## Add California Stream Condition Index

--> For next round of analytic revisions, look upstream and downstream along
    flowline for CSCI values, rather than averaging values within a buffer

```{r}
## create directory
if(!dir.exists(here("raw_data", "csci"))) {
  dir.create(here("raw_data", "csci"))
}

# specify file url
csci_url <- "https://indicators.ucdavis.edu/cwip/sites/default/files/data/indicator_59452/data_59452_all.csv"

# download file
if(!file.exists(here("raw_data", "csci", "csci_all.csv"))) {
  download.file(url = csci_url,
                destfile = here("raw_data", "csci", "csci_all.csv"))
}

# read in CSCI file
csci <- read_csv(here("raw_data", "csci", "csci_all.csv"),
                     col_names = TRUE) %>%
  rename(index = "...1") %>%
  group_by(station_code) %>%
  summarize(csci_score = min(csci_score, na.rm = TRUE),
            cwip_score = cwip_score[which.min(csci_score)],
            stream_name = stream_name[which.min(csci_score)],
            station_code = station_code[which.min(csci_score)],
            sample_date = sample_date[which.min(csci_score)],
            latitude = latitude[which.min(csci_score)],
            longitude = longitude[which.min(csci_score)]) %>%
  rowid_to_column(var = "csci_index") %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = global_crs)

# calculate the average CSCI score for all restoration sites
# using all CSCI observations within 10km of a restoration site
# aka intersecting a 10km buffer
csci_agg <- aggregate(dplyr::select(csci, csci_score, cwip_score),
                      dplyr::select(study_points, obs_id),
                      FUN = mean, 
                      join = function(x, y) st_is_within_distance(x, y, dist = set_units(10, "km"))) %>%
  st_drop_geometry()

# bind CSCI aggregated values to restoration projects
study_points <- cbind(study_points, csci_agg)

# clean up
rm(csci_url, csci_agg, csci)
```

## Add nitrates

--> For next round of analytic revisions, look upstream and downstream along
    flowline for nitrate values, rather than averaging values within a buffer

```{r}
# create directory
if(!dir.exists(here("raw_data", "nitrates"))) {
  dir.create(here("raw_data", "nitrates"))
}

# specify url
nitrates_url <- "https://indicators.ucdavis.edu/cwip/sites/default/files/data/indicator_32232/data_32232_all.csv"

# download file
if(!file.exists(here("raw_data", "nitrates", "nitrates_all.csv"))) {
  download.file(url = nitrates_url,
                destfile = here("raw_data", "nitrates", "nitrates_all.csv"))
}

# read in nitrates file
nitrates <- read_csv(here("raw_data", "nitrates", "nitrates_all.csv"),
                     col_names = TRUE) %>%
  rename(index = "...1") %>%
  group_by(code) %>%
  summarize(nitrate_score = min(score, na.rm = TRUE),
            name = name[which.min(score)],
            code = code[which.min(score)],
            date = date[which.min(score)],
            lat = lat[which.min(score)],
            lon = lon[which.min(score)]) %>%
  st_as_sf(coords = c("lon", "lat"), crs = global_crs)


# calculate the average nitrates score for all restoration sites
# using all nitrates observations within 10km of a restoration site
# aka intersecting a 10km buffer
nitrates_agg <- aggregate(dplyr::select(nitrates, nitrate_score),
                          dplyr::select(study_points, obs_id),
                          FUN = mean, 
                          join = function(x, y) st_is_within_distance(x, y, dist = set_units(10, "km"))) %>%
  st_drop_geometry()

# bind CSCI aggregated values to restoration projects
study_points <- cbind(study_points, nitrates_agg)

# clean up
rm(nitrates_url, nitrates_agg, nitrates)
```

## Add 303d listed waterbodies

```{r warning = FALSE}
# create directory
if(!dir.exists(here("raw_data", "303d"))) {
  dir.create(here("raw_data", "303d"))
}

# specify url
waterbodies_303d_lines_url <- "https://gispublic.waterboards.ca.gov/webmap/303d_2014_2016/files/IR_1416_Impaired_Lines.zip"

# download file
if(!file.exists(here("raw_data", "303d", "IR_1416_Impaired_Lines.zip"))) {
  download.file(url = waterbodies_303d_lines_url,
                destfile = here("raw_data", "303d", "IR_1416_Impaired_Lines.zip"))
}

# unzip file
unzip(here("raw_data", "303d", "IR_1416_Impaired_Lines.zip"),
      exdir = here("raw_data", "303d"))

# specify url
waterbodies_303d_polygons_url <- "https://gispublic.waterboards.ca.gov/webmap/303d_2014_2016/files/IR_1416_Impaired_Polys.zip"

# download file
if(!file.exists(here("raw_data", "303d", "IR_1416_Impaired_Polys.zip"))) {
  download.file(url = waterbodies_303d_polygons_url,
                destfile = here("raw_data", "303d", "IR_1416_Impaired_Polys.zip"))
}

# unzip file
unzip(here("raw_data", "303d", "IR_1416_Impaired_Polys.zip"),
      exdir = here("raw_data", "303d"))

# read in
waterbodies_303d_lines <- st_read(here("raw_data", "303d", "IR_1416_Impaired_Lines")) %>%
  st_transform(crs = global_crs) %>%
  ms_simplify(keep = 0.05) %>%
  mutate(waterbody_303d = TRUE) %>%
  dplyr::select(waterbody_303d)

waterbodies_303d_polygons <- st_read(here("raw_data", "303d", "IR_1416_Impaired_Polys")) %>%
  st_transform(crs = global_crs) %>%
  ms_simplify(keep = 0.05) %>%
  mutate(waterbody_303d = TRUE,
         is_valid = st_is_valid(geometry)) %>%
  filter(is_valid) %>%
  dplyr::select(waterbody_303d)

# buffer restoration sites for intersection with 303d listed waterbodies
study_points_buffered <- st_buffer(study_points, dist = set_units(10, "km")) %>%
  dplyr::select(obs_id)

# intersect buffered restoration sites with 303d listed waterbodies
study_points_buffered_303d <- study_points_buffered %>%
  st_join(y = waterbodies_303d_lines) %>%
  st_join(y = waterbodies_303d_polygons) %>%
  st_drop_geometry() %>%
  group_by(obs_id) %>%
  summarize(streams_303d = max(waterbody_303d.x, na.rm = TRUE),
            lakes_ponds_303d = max(waterbody_303d.x, na.rm = TRUE))

# clean results
study_points_buffered_303d <- do.call(data.frame,
                                      lapply(study_points_buffered_303d,
                                             function(x) replace(x, is.infinite(x), NA)))

# join to restoration projects dataframe
study_points <- left_join(study_points, study_points_buffered_303d,
                          by = "obs_id")

# replace 303d NA values with 0
study_points$streams_303d[is.na(study_points$streams_303d)] <- 0
study_points$lakes_ponds_303d[is.na(study_points$lakes_ponds_303d)] <- 0

# create a single 303d column that combine stream, lakes, and ponds statuses
study_points <- study_points %>%
  rowwise() %>%
  mutate(any_303d = max(streams_303d, lakes_ponds_303d)) %>%
  ungroup()

# clean up
rm(waterbodies_303d_lines_url, waterbodies_303d_polygons_url,
   waterbodies_303d_lines, waterbodies_303d_polygons, study_points_buffered_303d)
```

## Add in anadromous fish distribution data

```{r}
# specify whether to include fish data
fish <- FALSE
```

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist"))) {
    dir.create(here("raw_data", "anadromous_dist"))
  }
}
```

### Import coho

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist", "coho"))) {
    dir.create(here("raw_data", "anadromous_dist", "coho"))
  }
  
  # specify coho url
  coho_url <- "ftp://ftp.streamnet.org/pub/calfish/Coho_Distribution_and_Range_Jun16.zip"
  
  # download coho data
  if(!file.exists(here("raw_data", "anadromous_dist", "coho", "coho.zip"))) {
    download.file(url = coho_url,
                  destfile = here("raw_data", "anadromous_dist", "coho", "coho.zip"))
  }
  
  # unzip coho data
  unzip(zipfile = here("raw_data", "anadromous_dist", "coho", "coho.zip"),
        exdir = here("raw_data", "anadromous_dist", "coho"))
  
  # read in coho data and clean
  coho <- readOGR(dsn = here("raw_data", "anadromous_dist", "coho", "ds534", "ds534.gdb")) %>%
    st_as_sf() %>%
    st_transform(crs = global_crs) %>%
    mutate(coho = TRUE) %>%
    dplyr::select(coho, geometry)
}
```

### Import coastal chinook

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist", "coastal_chinook"))) {
    dir.create(here("raw_data", "anadromous_dist", "coastal_chinook"))
  }
  
  # specify coastal chinook url
  coastal_chinook_url <- "https://filelib.wildlife.ca.gov/Public/BDB/GIS/BIOS/Public_Datasets/900_999/ds981.zip"
  
  # download coastal chinook data
  if(!file.exists(here("raw_data", "anadromous_dist", "coastal_chinook", "coastal_chinook.zip"))) {
    download.file(url = coastal_chinook_url,
                  destfile = here("raw_data", "anadromous_dist", "coastal_chinook", "coastal_chinook.zip"))
  }
  
  # unzip coastal chinook data
  unzip(zipfile = here("raw_data", "anadromous_dist", "coastal_chinook", "coastal_chinook.zip"),
        exdir = here("raw_data", "anadromous_dist", "coastal_chinook"))
  
  # read in coastal chinook data and clean
  coastal_chinook <- st_read(here("raw_data", "anadromous_dist", "coastal_chinook")) %>%
    st_zm() %>%
    st_transform(crs = global_crs) %>%
    st_buffer(dist = set_units(1000, "m"))
  
  coastal_chinook <- st_union(coastal_chinook) %>%
    st_as_sf() %>%
    st_make_valid(geometry) %>%
    mutate(coastal_chinook = TRUE)
}
```

### Import spring-run Central Valley chinook

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist", "spring_cv_chinook"))) {
    dir.create(here("raw_data", "anadromous_dist", "spring_cv_chinook"))
  }
  
  # specify spring-run Central Valley chinook url
  spring_cv_chinook_url <- "https://filelib.wildlife.ca.gov/Public/BDB/GIS/BIOS/Public_Datasets/900_999/ds982.zip"
  
  # download spring-run Central Valley chinook data
  if(!file.exists(here("raw_data", "anadromous_dist", "spring_cv_chinook", "spring_cv_chinook.zip"))) {
    download.file(url = spring_cv_chinook_url,
                  destfile = here("raw_data", "anadromous_dist", "spring_cv_chinook", "spring_cv_chinook.zip"))
  }
  
  # download spring-run Central Valley chinook data
  unzip(zipfile = here("raw_data", "anadromous_dist", "spring_cv_chinook", "spring_cv_chinook.zip"),
        exdir = here("raw_data", "anadromous_dist", "spring_cv_chinook"))
  
  # read in coastal chinook data and clean
  spring_cv_chinook <- readOGR(dsn = here("raw_data", "anadromous_dist", "spring_cv_chinook", "ds982.gdb")) %>%
    st_as_sf() %>%
    st_transform(crs = global_crs) %>%
    st_buffer(dist = set_units(1000, "m"))
  
  spring_cv_chinook <- st_union(spring_cv_chinook) %>%
    st_as_sf() %>%
    st_make_valid(geometry) %>%
    mutate(spring_cv_chinook = TRUE)
}
```

### Import summer steelhead

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist", "summer_steelhead"))) {
    dir.create(here("raw_data", "anadromous_dist", "summer_steelhead"))
  }
  
  # specify summer steelhead url
  summer_steelhead_url <- "ftp://ftp.streamnet.org/pub/calfish/Summer_Steelhead_Distribution_and_Range_Oct09.zip"
  
  # download summer steelhead data
  if(!file.exists(here("raw_data", "anadromous_dist", "summer_steelhead", "summer_steelhead.zip"))) {
    download.file(url = summer_steelhead_url,
                  destfile = here("raw_data", "anadromous_dist", "summer_steelhead", "summer_steelhead.zip"))
  }
  
  # unzip summer steelhead data
  unzip(zipfile = here("raw_data", "anadromous_dist", "summer_steelhead", "summer_steelhead.zip"),
        exdir = here("raw_data", "anadromous_dist", "summer_steelhead"))
  
  # read in summer steelhead data and clean
  summer_steelhead <- st_read(here("raw_data", "anadromous_dist", "summer_steelhead"),
                              layer = "Summer_Steelhead_Range") %>%
    st_transform(global_crs) %>%
    mutate(summer_steelhead = TRUE) %>%
    dplyr::select(summer_steelhead, geometry)
}
```

### Import winter steelhead

```{r}
if(fish) {
  # create directory
  if(!dir.exists(here("raw_data", "anadromous_dist", "winter_steelhead"))) {
    dir.create(here("raw_data", "anadromous_dist", "winter_steelhead"))
  }
  
  # specify winter steelhead url
  winter_steelhead_url <- "ftp://ftp.streamnet.org/pub/calfish/Winter_Steelhead_Distribution_and_Range_Jun12.zip"
  
  # download winter steelhead data
  if(!file.exists(here("raw_data", "anadromous_dist", "winter_steelhead", "winter_steelhead.zip"))) {
    download.file(url = winter_steelhead_url,
                  destfile = here("raw_data", "anadromous_dist", "winter_steelhead", "winter_steelhead.zip"))
  }
  
  # unzip winter steelhead data
  unzip(zipfile = here("raw_data", "anadromous_dist", "winter_steelhead", "winter_steelhead.zip"),
        exdir = here("raw_data", "anadromous_dist", "winter_steelhead"))
  
  # read in winter steelhead data and clean
  winter_steelhead <- st_read(here("raw_data", "anadromous_dist", "winter_steelhead"),
                              layer = "Winter_Steelhead_Range") %>%
    st_transform(global_crs) %>%
    mutate(winter_steelhead = TRUE) %>%
    dplyr::select(winter_steelhead, geometry)
}
```

### Add all fish types to study points

```{r}
if(fish) {
  # intersect buffered points with fish sf objects
  study_points_buffered_fish <- study_points_buffered %>%
    st_join(coho) %>%
    st_join(coastal_chinook) %>%
    st_join(spring_cv_chinook) %>%
    st_join(summer_steelhead) %>%
    st_join(winter_steelhead) %>%
    st_drop_geometry()
  
  # clean up - this is challenging for memory
  rm(coho, coastal_chinook, spring_cv_chinook, summer_steelhead, winter_steelhead,
     coho_url, coastal_chinook_url, spring_cv_chinook_url,
     summer_steelhead_url, winter_steelhead_url)
  
  # clean duplicates introduced in spatial join
  study_points_buffered_fish <- study_points_buffered_fish %>%
    group_by(obs_id) %>%
    summarize(coho = max(coho, na.rm = TRUE),
              coastal_chinook = max(coastal_chinook, na.rm = TRUE),
              spring_cv_chinook = max(spring_cv_chinook, na.rm = TRUE),
              summer_steelhead = max(summer_steelhead, na.rm = TRUE),
              winter_steelhead = max(winter_steelhead, na.rm = TRUE)) %>%
    distinct()
  
  # add fish data to study points
  study_points <- left_join(study_points, study_points_buffered_fish, by = "obs_id") %>%
    rowwise() %>%
    mutate(n_anadromous = sum(coho, coastal_chinook, spring_cv_chinook,
                              summer_steelhead, winter_steelhead, na.rm = TRUE)) %>%
    ungroup()
  
  # clean up
  rm(study_points_buffered_fish)
}
```



# Import socioeconomic data

```{r}
# create directory
if(!dir.exists(here("cleaned_data", "ses_midprocess"))) {
  dir.create(here("cleaned_data", "ses_midprocess"))
}

# download midprocess SES data (see Jenny's script)
drive_download(file = as_id("1SWML4oqG-nT-wtN87c9Dn9PnyPAd1Lgo"),
               path = here("cleaned_data", "ses_midprocess", "ses_midprocess.gpkg"),
               overwrite = TRUE)

# read in midprocess SES data
ses <- st_read(here("cleaned_data", "ses_midprocess", "ses_midprocess.gpkg")) %>%
  st_transform(global_crs) %>%
  distinct(obs_id, .keep_all = TRUE) %>%
  dplyr::select(obs_id, total_pop_buff_area, nh_white_buff_area,
                mhi_wtd_buff_area, pctg_bipoc_buff_area)

# join SES data to study points
study_points <- left_join(study_points, st_drop_geometry(ses), by = "obs_id")

# clean up
rm(ses)
```



# Clean final study points attributes dataframe

```{r}
# interpolate/attend to missing values
# HERE!

# filter outliers
study_points <- study_points %>%
  filter(drainage_density < 3)
```



# Explore correlation between variables

```{r}
# list variables of interest for correlation evaluation
check_corr_vars <- c("slope", "drainage_density", "native_aquatic_index", "csci_score",
                     "precip", "cwip_score", "nitrate_score", "total_pop_buff_area",
                     "mhi_wtd_buff_area", "pctg_bipoc_buff_area")

if(fish) {
  check_corr_vars <- c(check_corr_vars, "n_anadromous")
}

# drop geometry from study points, select columns of interest, and make numeric
check_corr <- study_points %>%
  st_drop_geometry() %>%
  dplyr::select(check_corr_vars) %>%
  mutate(across(everything(), ~as.numeric(.)))

# grab only complete cases
check_corr <- check_corr[complete.cases(check_corr), ]

# make a Pearson's correlation matrix
cor(check_corr) %>%
  corrplot(.,
           method = "number",
           type = "upper")
```



# Create match data

## Set up datasets for matching

```{r}
# identify data of interest in matching
match_vars <- c("obs_id", "obs_type",
                "slope", "drainage_density", "precip",
                "native_aquatic_index", "csci_score", "any_303d",
                "total_pop_buff_area", "mhi_wtd_buff_area", "pctg_bipoc_buff_area")

# create dataframe of data of interest for matching
to_match <- study_points %>%
  st_drop_geometry() %>%
  dplyr::select(match_vars) %>%
  mutate(obs_type_bin = case_when(obs_type == "control" ~ 0,
                                  obs_type == "restoration_site" ~ 1))

# keep only complete cases
to_match <- to_match[complete.cases(to_match), ]
```

## Do the matching

```{r}
# match on nearest neighbor
match_nearestneighbor <- matchit(obs_type_bin ~ slope + drainage_density +
                                   native_aquatic_index + precip + csci_score +
                                   any_303d + total_pop_buff_area, 
                                 data = to_match,
                                 method = "nearest",
                                 distance = "glm")

# pull out dataframe of match results
match_result <- match.data(match_nearestneighbor)
```



# Have fun with stats

## Run conditional logistic regression

```{r}
# run conditional logistic regression with clogit
match_logregression <- clogit(obs_type_bin ~ mhi_wtd_buff_area + pctg_bipoc_buff_area + strata(subclass),
                              data = match_result)
```

## Check spatial autocorrelation

### Join in geometry

```{r}
# join geometry to matched points
match_result_sf <- study_points %>%
  dplyr::select(obs_id, geometry) %>%
  inner_join(match_result, by = "obs_id")

# map match result
ggplot() +
  geom_sf(data = ca_boundary) +
  geom_sf(data = match_result_sf, aes(color = obs_type), size = 0.8) +
  theme_minimal()
```

### Test Moran's i

```{r}
```

